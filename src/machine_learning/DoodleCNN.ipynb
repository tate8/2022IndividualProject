{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPjlC4WZQZWl"
   },
   "source": [
    "# Doodle CNN\n",
    "A neural network to predict what was drawn, given 28x28 input picture pixel data\\\n",
    "Trained with quickdraw-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqE0ZZB-N-bG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import zipfile\n",
    "from contextlib import ExitStack\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from functools import partial\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXj77Ev8OBGV"
   },
   "source": [
    "### Step 1: Get TFRecord dataset filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__) # make sure 2.4.1 for compatibility with server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHKdTTItQfvm"
   },
   "source": [
    "Get all data from .npy files, store in tensoflow Dataset object. Shuffle the data, and save to a .tfrecord file for easy access in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ahr1_IlkOivL"
   },
   "outputs": [],
   "source": [
    "def create_example_protobuff(image, label):\n",
    "    # convert to binary string format for Example protobuf\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "\n",
    "    return tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature={\n",
    "                'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_data.numpy()])),\n",
    "                'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xKE-uSAOkK0"
   },
   "outputs": [],
   "source": [
    "def write_tfrecords(name, dataset):\n",
    "    path = f'{name}.tfrecord'\n",
    "    with ExitStack() as stack:\n",
    "        writer = stack.enter_context(tf.io.TFRecordWriter(path))\n",
    "        \n",
    "        # create example protobuffs from instances\n",
    "        for image, label in dataset:\n",
    "            example = create_example_protobuff(image, np.uint8(label))\n",
    "            writer.write(example.SerializeToString())\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtmQ1YscOl3X",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_files = glob.glob(\"data/*\")\n",
    "\n",
    "def load_data():\n",
    "    class_names = []\n",
    "    all_filepaths = []\n",
    "    num_files = 0\n",
    "\n",
    "    # load each data file \n",
    "    for idx, file in enumerate(all_files):\n",
    "        data = np.load(file)\n",
    "        # data is 784, but need to reshape to 28x28 for CNN\n",
    "        data = data.reshape((data.shape[0], 28, 28)).astype(np.uint8)\n",
    "        labels = np.full(data.shape[0], idx)\n",
    "        \n",
    "        # convert numpy array to Tensorflow Dataset object\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "\n",
    "        # class name will be name of file e.g. 'fork.npy' is 'fork'\n",
    "        class_name, ext = os.path.splitext(os.path.basename(file))\n",
    "        class_names.append(class_name)\n",
    "        print(class_name)\n",
    "        \n",
    "        # write Dataset to files\n",
    "        filepaths = write_tfrecords(f\"doodle-{class_name}\", dataset)\n",
    "        all_filepaths.append(filepaths)\n",
    "        \n",
    "        # logging\n",
    "        num_files += 1\n",
    "        # every 35 files\n",
    "        if num_files % 35 == 0:\n",
    "            print(f'{num_files} file npy to tfrecord')\n",
    "        \n",
    "    return all_filepaths, class_names\n",
    "    \n",
    "filepaths, class_names = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --For Google Colab---\n",
    "# for tfrecord files already created from the .npy files\n",
    "# in my case, located in google drive: /content/drive/MyDrive/DoodleData\n",
    "\n",
    "\n",
    "paths = ['data/lollipop.npy', 'data/binoculars.npy', 'data/garden.npy', 'data/basket.npy', 'data/penguin.npy', 'data/washing machine.npy', 'data/canoe.npy', 'data/eyeglasses.npy', 'data/beach.npy', 'data/screwdriver.npy', 'data/mouse.npy', 'data/apple.npy', 'data/van.npy', 'data/grapes.npy', 'data/grass.npy', 'data/watermelon.npy', 'data/floor lamp.npy', 'data/moon.npy', 'data/zigzag.npy', 'data/nail.npy', 'data/leg.npy', 'data/rollerskates.npy', 'data/goatee.npy', 'data/sun.npy copy', 'data/cup.npy', 'data/anvil.npy', 'data/suitcase.npy', 'data/chair.npy', 'data/drill.npy', 'data/peanut.npy', 'data/squirrel.npy', 'data/matches.npy', 'data/sword.npy', 'data/cat.npy', 'data/toe.npy', 'data/snorkel.npy', 'data/pond.npy', 'data/calculator.npy', 'data/airplane.npy', 'data/squiggle.npy', 'data/blackberry.npy', 'data/ear.npy', 'data/frying pan.npy', 'data/chandelier.npy', 'data/tree.npy', 'data/wine bottle.npy', 'data/peas.npy', 'data/hot tub.npy', 'data/door.npy', 'data/calendar.npy', 'data/wine glass.npy', 'data/stove.npy', 'data/hockey stick.npy', 'data/toothpaste.npy', 'data/moustache.npy', 'data/mountain.npy', 'data/tooth.npy', 'data/firetruck.npy', 'data/cannon.npy', 'data/stereo.npy', 'data/shorts.npy', 'data/cloud.npy', 'data/paintbrush.npy', 'data/pear.npy', 'data/frog.npy', 'data/laptop.npy', 'data/dishwasher.npy', 'data/vase.npy', 'data/diving board.npy', 'data/octagon.npy', 'data/smiley face.npy', 'data/dumbbell.npy', 'data/sweater.npy', 'data/stitches.npy', 'data/tractor.npy', 'data/foot.npy', 'data/basketball.npy', 'data/helmet.npy', 'data/crab.npy', 'data/clock.npy', 'data/diamond.npy', 'data/car.npy', 'data/axe.npy', 'data/traffic light.npy', 'data/sleeping bag.npy', 'data/baseball.npy', 'data/eye.npy', 'data/flower.npy', 'data/hot air balloon.npy', 'data/waterslide.npy', 'data/coffee cup.npy', 'data/bottlecap.npy', 'data/banana.npy', 'data/dresser.npy', 'data/house plant.npy', 'data/skyscraper.npy', 'data/skateboard.npy', 'data/pizza.npy', 'data/hammer.npy', 'data/teapot.npy', 'data/giraffe.npy', 'data/underwear.npy', 'data/snowman.npy', 'data/monkey.npy', 'data/computer.npy', 'data/pencil.npy', 'data/shovel.npy', 'data/knife.npy', 'data/bat.npy', 'data/compass.npy', 'data/necklace.npy', 'data/bicycle.npy', 'data/teddy-bear.npy', 'data/bucket.npy', 'data/line.npy', 'data/bus.npy', 'data/cello.npy', 'data/ocean.npy', 'data/truck.npy', 'data/camouflage.npy', 'data/harp.npy', 'data/stairs.npy', 'data/telephone.npy', 'data/star.npy', 'data/guitar.npy', 'data/sandwich.npy', 'data/sun.npy', 'data/feather.npy', 'data/leaf.npy', 'data/toilet.npy', 'data/strawberry.npy', 'data/birthday cake.npy', 'data/saxophone.npy', 'data/rake.npy', 'data/broom.npy', 'data/stethoscope.npy', 'data/square.npy', 'data/crown.npy', 'data/fire hydrant.npy', 'data/donut.npy', 'data/jail.npy', 'data/oven.npy', 'data/beard.npy', 'data/syringe.npy', 'data/yoga.npy', 'data/The Eiffel Tower.npy', 'data/camera.npy', 'data/purse.npy', 'data/ice cream.npy', 'data/pig.npy', 'data/trumpet.npy', 'data/table.npy', 'data/bush.npy', 'data/scorpion.npy', 'data/fish.npy', 'data/hot dog.npy', 'data/see saw.npy', 'data/rain.npy', 'data/snail.npy', 'data/sink.npy', 'data/belt.npy', 'data/speedboat.npy', 'data/trombone.npy', 'data/pants.npy', 'data/crocodile.npy', 'data/broccoli.npy', 'data/hedgehog.npy', 'data/rainbow.npy', 'data/bulldozer.npy', 'data/fork.npy', 'data/sock.npy', 'data/snake.npy', 'data/paper clip.npy', 'data/bear.npy', 'data/marker.npy', 'data/tent.npy', 'data/rabbit.npy', 'data/clarinet.npy', 'data/whale.npy', 'data/boomerang.npy', 'data/hospital.npy', 'data/ceiling fan.npy', 'data/pillow.npy', 'data/saw.npy', 'data/fence.npy', 'data/parrot.npy', 'data/duck.npy', 'data/dog.npy', 'data/swing set.npy', 'data/spoon.npy', 'data/fan.npy', 'data/cruise ship.npy', 'data/picture frame.npy', 'data/mushroom.npy', 'data/headphones.npy', 'data/horse.npy', 'data/flying saucer.npy', 'data/skull.npy', 'data/rifle.npy', 'data/train.npy', 'data/hat.npy', 'data/mouth.npy', 'data/book.npy', 'data/drums.npy', 'data/radio.npy', 'data/roller coaster.npy', 'data/snowflake.npy', 'data/piano.npy', 'data/rhinoceros.npy', 'data/cake.npy', 'data/paint can.npy', 'data/toaster.npy', 'data/knee.npy', 'data/spider.npy', 'data/sea turtle.npy', 'data/popsicle.npy', 'data/pickup truck.npy', 'data/envelope.npy', 'data/remote control.npy', 'data/ambulance.npy', 'data/pliers.npy', 'data/bread.npy', 'data/castle.npy', 'data/river.npy', 'data/bandage.npy', 'data/lion.npy', 'data/postcard.npy', 'data/bench.npy', 'data/parachute.npy', 'data/keyboard.npy', 'data/streetlight.npy', 'data/arm.npy', 'data/police car.npy', 'data/sailboat.npy', 'data/cooler.npy', 'data/bathtub.npy', 'data/hurricane.npy', 'data/campfire.npy', 'data/soccer ball.npy', 'data/potato.npy', 'data/dolphin.npy', 'data/key.npy', 'data/elephant.npy', 'data/tornado.npy', 'data/jacket.npy', 'data/nose.npy', 'data/motorbike.npy', 'data/octopus.npy', 'data/bracelet.npy', 'data/brain.npy', 'data/toothbrush.npy', 'data/The Mona Lisa.npy', 'data/carrot.npy', 'data/barn.npy', 'data/zebra.npy', 'data/microphone.npy', 'data/map.npy', 'data/camel.npy', 'data/wheel.npy', 'data/bridge.npy', 'data/lighthouse.npy', 'data/spreadsheet.npy', 'data/hockey puck.npy', 'data/wristwatch.npy', 'data/helicopter.npy', 'data/swan.npy', 'data/flamingo.npy', 'data/backpack.npy', 'data/lobster.npy', 'data/golf club.npy', 'data/hexagon.npy', 'data/garden hose.npy', 'data/bird.npy', 'data/animal migration.npy', 'data/finger.npy', 'data/steak.npy', 'data/mailbox.npy', 'data/shark.npy', 'data/television.npy', 'data/mermaid.npy', 'data/cow.npy', 'data/crayon.npy', 'data/palm tree.npy', 'data/windmill.npy', 'data/cookie.npy', 'data/kangaroo.npy', 'data/blueberry.npy', 'data/tennis racquet.npy', 'data/tiger.npy', 'data/dragon.npy', 'data/cell phone.npy', 'data/pineapple.npy', 'data/sheep.npy', 'data/candle.npy', 'data/angel.npy', 'data/cactus.npy', 'data/mosquito.npy', 'data/couch.npy', 'data/church.npy', 'data/The Great Wall of China.npy', 'data/hamburger.npy', 'data/school bus.npy', 'data/lipstick.npy', 'data/light bulb.npy', 'data/flip flops.npy', 'data/alarm clock.npy', 'data/aircraft carrier.npy', 'data/face.npy', 'data/ant.npy', 'data/microwave.npy', 'data/hourglass.npy', 'data/panda.npy', 'data/pool.npy', 'data/circle.npy', 'data/onion.npy', 'data/lighter.npy', 'data/raccoon.npy', 'data/bowtie.npy', 'data/umbrella.npy', 'data/butterfly.npy', 'data/fireplace.npy', 'data/eraser.npy', 'data/bee.npy', 'data/flashlight.npy', 'data/megaphone.npy', 'data/asparagus.npy', 'data/shoe.npy', 'data/ladder.npy', 'data/t-shirt.npy', 'data/passport.npy', 'data/triangle.npy', 'data/hand.npy', 'data/lightning.npy', 'data/mug.npy', 'data/submarine.npy', 'data/violin.npy', 'data/owl.npy', 'data/scissors.npy', 'data/string bean.npy', 'data/baseball bat.npy', 'data/lantern.npy', 'data/house.npy', 'data/elbow.npy', 'data/power outlet.npy', 'data/stop sign.npy', 'data/bed.npy']\n",
    "class_names = []\n",
    "for path in paths:\n",
    "    class_name, ext = os.path.splitext(os.path.basename(path))\n",
    "    class_names.append(class_name)\n",
    "\n",
    "\n",
    "# filepaths are all file located in tfrecord dir\n",
    "filepaths = glob.glob(\"tfrecords/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAFxxFvtOo2l",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DoodleDataset:\n",
    "  '''\n",
    "    Create TFRecordDataset from filepaths\n",
    "  '''\n",
    "  def __init__(self, filepaths, shuffle_buffer_size, batch_size=128):\n",
    "    self.filepaths = filepaths\n",
    "    self.shuffle_buffer_size = shuffle_buffer_size\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  # parse serialized Example protobuf\n",
    "  def preprocess(self, tfrecord):\n",
    "    # to parse we need the feature description of the protobuf\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64, default_value=1)\n",
    "    }\n",
    "    parsed_example = tf.io.parse_single_example(tfrecord, feature_description)\n",
    "    image = tf.io.parse_tensor(parsed_example['image'], out_type=tf.uint8)\n",
    "    # now reshape\n",
    "    image = tf.reshape(image, [28, 28])\n",
    "    return image, parsed_example['label']\n",
    "\n",
    "\n",
    "  def create_dataset(self):\n",
    "    # reading all filepaths in parallel\n",
    "    dataset = tf.data.TFRecordDataset(self.filepaths, num_parallel_reads=len(self.filepaths))\n",
    "\n",
    "    # shuffling\n",
    "    dataset = dataset.shuffle(self.shuffle_buffer_size)\n",
    "\n",
    "    # parse serialized Dataset\n",
    "    dataset = dataset.map(self.preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(self.batch_size)\n",
    "    # be 1 batch ahead\n",
    "    return dataset.prefetch(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjNvMQ9QPlZk"
   },
   "outputs": [],
   "source": [
    "# create doodle dataset object\n",
    "# need good shuffling so set the shuffle buffer size very large (even though it takes longer)\n",
    "doodle_dataset = DoodleDataset(filepaths, shuffle_buffer_size=4000000)\n",
    "\n",
    "full_set = doodle_dataset.create_dataset()\n",
    "\n",
    "val_set = full_set.take(50000)\n",
    "\n",
    "tmp_set = full_set.skip(50000)\n",
    "\n",
    "test_set = tmp_set.take(50000)\n",
    "\n",
    "# rest\n",
    "train_set = tmp_set.skip(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(filepath):\n",
    "  # reading all filepaths in parallel\n",
    "    dataset = tf.data.TFRecordDataset(filepath)\n",
    "    \n",
    "    dataset = dataset.shuffle(4000)\n",
    "\n",
    "  # parse serialized Dataset\n",
    "    dataset = dataset.map(doodle_dataset.preprocess)\n",
    "  # be 1 batch ahead\n",
    "    return dataset.prefetch(1)\n",
    "  \n",
    "datasets = []\n",
    "for filepath in filepaths:\n",
    "    dataset = create_dataset(filepath)\n",
    "    datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate label data\n",
    "Go through different samples of all 346 classes and pick a image from each class to show the user when they click the help button on the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_label_data():\n",
    "    label_data = dict()\n",
    "    \n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        satisfied = False\n",
    "        while not satisfied:\n",
    "            for (x, y) in dataset.take(1):\n",
    "                x_npy = np.array(x)\n",
    "                filepath = filepaths[idx]\n",
    "                class_name, ext = os.path.splitext(os.path.basename(filepath))\n",
    "                class_name = class_name[7:] # trim off 'doodle-'\n",
    "                # display pixel data of sample\n",
    "                for i in range(28):\n",
    "                    for j in range(28):\n",
    "                        if x_npy[i, j] < 10:\n",
    "                            print(x_npy[i, j], end=\"\")\n",
    "                            print(\"   \", end=\"\")\n",
    "                        elif x_npy[i, j] < 100:\n",
    "                            print(x_npy[i, j], end=\"\")\n",
    "                            print(\"  \", end=\"\")\n",
    "                        else:\n",
    "                            print(x_npy[i, j], end=\"\")\n",
    "                            print(\" \", end=\"\")\n",
    "                    print('')\n",
    "                \n",
    "                question = input(f\"Are you satisfied with {class_name}? (y/N): \")\n",
    "                if question == 'y':\n",
    "                    label_data[class_name] = x_npy.tolist()\n",
    "                    satisfied = True\n",
    "    return label_data\n",
    "\n",
    "\n",
    "\n",
    "label_data = get_label_data()\n",
    "with open('label_data.json', 'w') as f:\n",
    "  # put data into file\n",
    "  json.dump(label_data, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qI6Tp6a_9ezk"
   },
   "source": [
    "### Now that we have and can save and load from tfrecord files, lets see some example images that we loaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "executionInfo": {
     "elapsed": 73723,
     "status": "ok",
     "timestamp": 1645107511151,
     "user": {
      "displayName": "Tate Larkin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04491054356615494518"
     },
     "user_tz": 420
    },
    "id": "x6zdQOnE9ezl",
    "outputId": "47db6964-78ce-4071-d53e-d0cd65c38c50",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for (X, y) in test_set.take(1):\n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(X[i].numpy(), cmap=\"binary\")\n",
    "        \n",
    "        plt.axis(\"off\")\n",
    "        plt.title(class_names[y[i].numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1JO5EIAWaif"
   },
   "source": [
    "## Step 2\n",
    "Machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEe3bO629ezl"
   },
   "outputs": [],
   "source": [
    "# clear session from possible previous models\n",
    "keras.backend.clear_session()\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# standardization\n",
    "# for each feature, subract the mean and divide by standard deviation\n",
    "# epsilon for divide by 0\n",
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())\n",
    "\n",
    "standardization = Standardization(input_shape=[28, 28, 1])\n",
    "\n",
    "# have to adapt to dataset\n",
    "# this will allow it to use the right mean and std dev for each feature\n",
    "sample_image_batches = train_set.take(1000).map(lambda image, label: image)\n",
    "sample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()),\n",
    "                               axis=0).astype(np.float32)\n",
    "standardization.adapt(sample_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4plC-d39ezm"
   },
   "source": [
    "### Architecture\n",
    "\n",
    "Using Google's ResNet-34 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8o-SrN49ezm"
   },
   "outputs": [],
   "source": [
    "# creates a new partial class 'DefaultConv2D' with starting params of a keras.layers.Conv2D\n",
    "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,\n",
    "                        padding=\"SAME\", use_bias=False)\n",
    "\n",
    "class ResidualUnit(keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.filters=filters\n",
    "        self.strides=strides\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            keras.layers.BatchNormalization()]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                keras.layers.BatchNormalization()]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'filters': self.filters,\n",
    "            'strides': self.strides,\n",
    "            'activation': self.activation,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Reshape((28, 28, 1), input_shape=(28, 28)))\n",
    "\n",
    "model.add(DefaultConv2D(64, kernel_size=7, strides=2))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(\"relu\"))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\"))\n",
    "prev_filters = 64\n",
    "\n",
    "# 3 ResidualUnits with 64 feature maps, then 4 ResidualUnits with 128 feature maps, then . . .\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "model.add(keras.layers.GlobalAvgPool2D())\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(346, activation=\"softmax\"))\n",
    " \n",
    "    \n",
    "    \n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUugc_Nj9ez3"
   },
   "outputs": [],
   "source": [
    "model_filepath = \"/content/drive/MyDrive/DoodleData/my_doodle_model.h5\"\n",
    "\n",
    "# create log directory for tensorboard\n",
    "logs = os.path.join(os.curdir, \"my_logs\", \"run_\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=logs, histogram_freq=1, profile_batch=10, update_freq=10000)\n",
    "# stop if model doesn't improve after 2 epochs\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=2)\n",
    "# save best model after each epoch\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath=model_filepath,\n",
    "                                                      save_best_only=True,\n",
    "                                                      save_freq='epoch',\n",
    "                                                      monitor='accuracy')\n",
    "\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGKyrpgG9ez4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_set, epochs=5, verbose=1, validation_data=val_set, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f8Moq9T9e0E"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM7ToHr3/L8+W12iS2M20Do",
   "collapsed_sections": [],
   "name": "DoodleCNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "generateTFRecordVenv",
   "language": "python",
   "name": "generatetfrecordvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
