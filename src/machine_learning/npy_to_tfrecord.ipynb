{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from contextlib import ExitStack\n",
    "import glob\n",
    "import os\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from umap import UMAP\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example_protobuff(image, label):\n",
    "    # convert to binary string format for Example protobuf\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "\n",
    "    return tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature={\n",
    "                'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_data.numpy()])),\n",
    "                'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecords(name, dataset):\n",
    "#     num_examples_in_smaller_file = 3_000\n",
    "    full_path = f'doodle-full-clean/{name}.tfrecord'\n",
    "#     small_path = f'doodle-small-clean/{name}.tfrecord'\n",
    "#     small_dataset = dataset.take(num_examples_in_smaller_file)\n",
    "\n",
    "    with ExitStack() as stack:\n",
    "        writer = stack.enter_context(tf.io.TFRecordWriter(full_path))\n",
    "\n",
    "        # create example protobuffs from instances\n",
    "        for image, label in dataset:\n",
    "            example = create_example_protobuff(image, np.uint8(label))\n",
    "            writer.write(example.SerializeToString())\n",
    "\n",
    "#     with ExitStack() as stack:\n",
    "#         writer = stack.enter_context(tf.io.TFRecordWriter(small_path))\n",
    "\n",
    "#         # create example protobuffs from instances\n",
    "#         for image, label in small_dataset:\n",
    "#             example = create_example_protobuff(image, np.uint8(label))\n",
    "#             writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(X):\n",
    "    x_tfm = UMAP(n_components=2).fit_transform(X)\n",
    "    \n",
    "    gm = GaussianMixture(n_components=1, n_init=10)\n",
    "    gm.fit(x_tfm)\n",
    "    \n",
    "    # Any isntance located in a low-density region is considered to be an anomaly\n",
    "    densities = gm.score_samples(x_tfm) # score_samples esitmates the density of the model at any given location\n",
    "    # say 10% are anomalies (see https://koaning.io/til/moar-bad-labels/)\n",
    "    density_threshold = np.percentile(densities, 10) \n",
    "    non_anomalies_idxs = np.nonzero(densities > density_threshold)[0]\n",
    "    \n",
    "    X_clean = X[non_anomalies_idxs]\n",
    "    \n",
    "    return X_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345 file npy to tfrecord\n"
     ]
    }
   ],
   "source": [
    "all_files = sorted([path.lower() for path in glob.glob(\"doodle_data_npy/*\")])\n",
    "files_already_done = glob.glob(\"doodle-full-clean/*\")\n",
    "files_already_done = [\"doodle_data_npy/\" + file.split(\"/\")[1].split(\".\")[0] + \".npy\" for file in files_already_done]\n",
    "all_files = [file for file in all_files if file not in files_already_done]\n",
    "\n",
    "print(len(all_files))\n",
    "\n",
    "def load_data():\n",
    "    class_names = []\n",
    "    num_files = 0\n",
    "\n",
    "    # load each data file \n",
    "    for idx, file in enumerate(all_files):\n",
    "        data = np.load(file)\n",
    "        \n",
    "        ##################################\n",
    "        # Here we do the anomaly detection to clean the dataset\n",
    "        ##################################\n",
    "        data = clean_data(data)\n",
    "    \n",
    "        # data is 784, but need to reshape to 28x28 for CNN\n",
    "        data = data.reshape((data.shape[0], 28, 28)).astype(np.uint8)\n",
    "        labels = np.full(data.shape[0], idx)\n",
    "        \n",
    "        # convert numpy array to Tensorflow Dataset object\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "\n",
    "        # class name will be name of file e.g. 'fork.npy' is 'fork'\n",
    "        class_name, ext = os.path.splitext(os.path.basename(file))\n",
    "        class_names.append(class_name)\n",
    "        \n",
    "        # write Dataset to files\n",
    "        write_tfrecords(f\"{class_name}\", dataset)\n",
    "        \n",
    "        # logging\n",
    "        num_files += 1\n",
    "        clear_output(wait=True)\n",
    "        print(f'{num_files} file npy to tfrecord', flush=True)\n",
    "            \n",
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "283887b2a5ebd5c43f729d1117e54ef07448633f92214030607145941c4c3a64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
